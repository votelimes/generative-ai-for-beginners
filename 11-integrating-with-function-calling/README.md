# Интеграция с вызовом функций

![изображение урока](./images/11-lesson-banner.png?WT.mc_id=academic-105485-koreyst)

Вы усвоили немало информации в предыдущих уроках. Однако мы можем продолжить совершенствоваться. Некоторые вещи, с которыми мы можем справиться, - это получение более однородного формата ответа, чтобы упростить работу с ответом на следующих этапах. Кроме того, мы можем добавить данные из других источников, чтобы дополнить наше приложение.

Проблемы, упомянутые выше, и являются темой данной главы.

> **Видео скоро будет**

## Введение

В этом уроке будет рассмотрено:

- Объяснение, что такое вызов функции и в каких случаях он используется.
- Создание вызова функции с использованием Azure OpenAI.
- Как интегрировать вызов функции в приложение.

## Цели обучения

После завершения этого урока вы сможете:

- Объяснить цель использования вызова функции.
- Настроить вызов функции с использованием сервиса Azure OpenAI.
- Разработать эффективные вызовы функций для вашего приложения.

## Сценарий: улучшение нашего чатбота с помощью функций

В этом уроке мы хотим создать функцию для нашего стартапа в сфере образования, которая позволит пользователям использовать чатбота для поиска технических курсов. Мы будем рекомендовать курсы, соответствующие их уровню навыков, текущей роли и интересующей технологии.

Для выполнения этого сценария мы будем использовать комбинацию следующих инструментов:

- `Azure OpenAI` для создания чат-опыта для пользователя.
- `API-каталог Microsoft Learn` для помощи пользователям в поиске курсов на основе их запросов.
- `Вызов функции` для получения запроса пользователя и отправки его в функцию для выполнения запроса к API.

Давайте вначале рассмотрим, почему мы хотим использовать вызов функции:

## Почему вызов функции

До появления вызова функции ответы от LLM были неструктурированными и несогласованными. Разработчикам требовалось писать сложный код проверки, чтобы обрабатывать каждую вариацию ответа. Пользователи не могли получить ответы на вопросы вроде "Какая текущая погода в Стокгольме?". Это связано с тем, что модели ограничены данными, на которых было проведено обучение.

Вызов функции - это функциональность сервиса Azure OpenAI, которая позволяет преодолеть следующие ограничения:

- **Однородный формат ответа**. Если мы можем лучше контролировать формат ответа, мы сможем более легко интегрировать его с другими системами.
- **Внешние данные**. Возможность использовать данные из других источников в контексте чата приложения.

## Иллюстрирование проблемы через сценарий

> Мы рекомендуем вам использовать [блокнот](/11-integrating-with-function-calling/Lesson11-FunctionCalling.ipynb) Если вы хотите выполнить приведенный ниже сценарий, вы также можете просто читать его, поскольку мы пытаемся проиллюстрировать проблему, в которой функции могут помочь справиться с проблемой.

Давайте рассмотрим пример, иллюстрирующий проблему формата ответа:

Предположим, мы хотим создать базу данных с данными о студентах, чтобы мы могли предложить им подходящий курс. Ниже приведены два описания студентов, которые очень похожи по содержанию данных.

1. Установите соединение с нашим ресурсом Azure OpenAI:

   ```python
   import os
   import json
   from openai import AzureOpenAI
   from dotenv import load_dotenv
   load_dotenv()

   client = AzureOpenAI(
   api_key=os.environ['AZURE_OPENAI_KEY'],  # это также является значением по умолчанию и может быть опущено
   api_version = "2023-07-01-preview"
   )

   deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']
   ```

   Ниже приведен некоторый код на языке Python для настройки соединения с Azure OpenAI, где мы устанавливаем значения `api_type`, `api_base`, `api_version` и `api_key`.

2. Создание двух описаний студентов с использованием переменных `student_1_description` и `student_2_description`.

   ```python
   student_1_description="Эмили Джонсон - второкурсница, обучающаяся по специальности компьютерные науки в Дюкском университете. У нее средний балл 3.7. Эмили активно участвует в шахматном клубе и дебатном клубе университета. Она надеется сделать карьеру в области программной инженерии после окончания учебы."

   student_2_description = "Майкл Ли - второкурсник, обучающийся по специальности компьютерные науки в Стэнфордском университете. У него средний балл 3.8. Майкл известен своими навыками программирования и является активным членом робототехнического клуба университета. Он надеется сделать карьеру в области искусственного интеллекта после окончания учебы."
   ```

   Мы хотим отправить указанные выше описания студентов на обработку LLM для анализа данных. Эти данные могут быть использованы в нашем приложении и отправлены в API или сохранены в базе данных.

3. Создадим два идентичных запроса, в которых мы указываем LLM, какую информацию мы интересуемся:

   ```python
   prompt1 = f'''
   Пожалуйста, извлеките следующую информацию из данного текста и верните ее в виде JSON-объекта:

   имя
   специальность
   университет
   оценки
   клуб

   Это текст, из которого нужно извлечь информацию:
   {student_1_description}
   '''

   prompt2 = f'''
   Пожалуйста, извлеките следующую информацию из данного текста и верните ее в виде JSON-объекта:

   имя
   специальность
   университет
   оценки
   клуб

   Это текст, из которого нужно извлечь информацию:
   {student_2_description}
   '''
   ```

   В указанных выше запросах мы указываем LLM извлекать информацию и возвращать результат в формате JSON.
   
4. После настройки запросов и соединения с Azure OpenAI мы отправим запросы на обработку LLM с помощью `openai.ChatCompletion`. Мы сохраняем запрос в переменной `messages` и присваиваем роль `user`. Это сделано для имитации сообщения от пользователя, написанного чат-боту.

   ```python
   # Ответ на первый запрос
   openai_response1 = client.chat.completions.create(
   model=deployment,
   messages=[{'role': 'user', 'content': prompt1}]
   )
   openai_response1.choices[0].message.content

   # Ответ на второй запрос
   openai_response2 = client.chat.completions.create(
   model=deployment,
   messages=[{'role': 'user', 'content': prompt2}]
   )
   openai_response2.choices[0].message.content
   ```

   Теперь мы можем отправить оба запроса LLM и изучить полученный ответ, найдя его таким образом `openai_response1['choices'][0]['message']['content']`.

5. Наконец, мы можем преобразовать ответ в формат JSON, вызвав `json.loads`:

   ```python
   # Загрузка ответа в виде JSON-объекта
   json_response1 = json.loads(openai_response1.choices[0].message.content)
   json_response1
   ```

   Ответ 1:

   ```json
   { "name": "Emily Johnson", "major": "компьютерные науки", "school": "Дюкский университет", "grades": "3.7", "club": "Шахматный клуб" }
   ```

   Ответ 2:

   ```json
   { "name": "Michael Lee", "major": "компьютерные науки", "school": "Стэнфордский университет", "grades": "3.8 GPA", "club": "Робототехнический клуб" }
   ```

   Несмотря на то, что запросы одинаковые, а описания похожи, мы видим, что значения свойства `grades` отформатированы по-разному, например, может быть формат `3.7` или `3.7 GPA`.

   Это происходит потому, что LLM принимает неструктурированные данные в виде написанного запроса и также возвращает неструктурированные данные. Нам нужно иметь структурированный формат, чтобы знать, что ожидать при сохранении или использовании этих данных.

Как же мы решаем проблему форматирования? Используя функциональный вызов, мы можем убедиться, что получаем структурированные данные. При использовании функционального вызова LLM фактически не вызывает или выполняет какие-либо функции. Вместо этого мы создаем структуру, которую LLM должен следовать в своих ответах. Затем мы используем эти структурированные ответы, чтобы знать, какую функцию запускать в наших приложениях.

![поток функций](./images/Function-Flow.png?WT.mc_id=academic-105485-koreyst)

Затем мы можем взять то, что возвращает функция, и отправить это обратно в LLM. LLM затем отвечает, используя естественный язык, чтобы ответить на запрос пользователя.

## Примеры использования функционального вызова

Существует множество различных случаев использования, в которых функциональные вызовы могут улучшить ваше приложение, такие как:

- **Вызов внешних инструментов**. Чат-боты отлично подходят для предоставления ответов на вопросы пользователей. Используя функциональный вызов, чат-боты могут использовать сообщения от пользователей для выполнения определенных задач. Например, студент может попросить чат-бота "Отправить электронное письмо моему преподавателю, сказав, что мне нужна дополнительная помощь по этому предмету". Это может вызвать функцию `send_email(to: string, body: string)` для отправки электронного письма.

- **Создание запросов к API или базе данных**. Пользователи могут искать информацию, используя естественный язык, который преобразуется в форматированный запрос или запрос к API. Примером может быть запрос учителя "Кто из студентов сдал последнее задание", который может вызвать функцию с названием `get_completed(student_name: string, assignment: int, current_status: string)`.

- **Создание структурированных данных**. Пользователи могут взять блок текста или CSV и использовать LLM для извлечения важной информации из него. Например, студент может преобразовать статью из Википедии о соглашениях о мире, чтобы создать карточки с вопросами и ответами для обучения искусственного интеллекта. Это можно сделать с помощью функции под названием `get_important_facts(agreement_name: string, date_signed: string, parties_involved: list)`.

## Создание вашего первого функционального вызова

Процесс создания функционального вызова включает 3 основных шага:

1. **Вызов** API Chat Completions с указанием списка ваших функций и сообщения пользователя.
2. **Чтение** ответа модели для выполнения действия, например, выполнение функции или вызов API.
3. **Создание** еще одного вызова к API Chat Completions с ответом от вашей функции для использования этой информации и создания ответа пользователю.

![LLM Поток](./images/LLM-Flow.png?WT.mc_id=academic-105485-koreyst)

### Шаг 1 - создание сообщений

Первый шаг - создать сообщение пользователя. Его можно динамически присвоить, взяв значение из текстового поля, либо можно присвоить значение здесь. Если вы впервые работаете с API Chat Completions, нам необходимо определить `роль` и `содержание` сообщения.

`Роль` может быть `system` (создание правил), `assistant` (модель) или `user` (конечный пользователь). Для функционального вызова мы присваиваем роль `user` и задаем пример вопроса.

```python
messages = [{"role": "user", "content": "Найдите мне хороший курс для начинающего студента по изучению Azure."}]
```

Различные роли помогают LLM понять, говорит ли что-то система или пользователь, что помогает строить историю разговора, на которой LLM может основываться.

### Шаг 2 - создание функций

Затем мы определим функцию и параметры этой функции. Здесь мы будем использовать только одну функцию с именем `search_courses`, но вы можете создать несколько функций.

> **Важно**: Функции включаются в системное сообщение для LLM и будут включены в количество доступных токенов.
Below, we create the functions as an array of items. Each item is a function and has properties `name`, `description` and `parameters`:

```python
functions = [
   {
      "name": "search_courses",
      "description": "Извлекает курсы из индекса поиска на основе предоставленных параметров",
      "parameters": {
         "type": "object",
         "properties": {
            "role": {
               "type": "string",
               "description": "Роль учащегося (например, разработчик, специалист по обработке данных, студент и т. д.)"
            },
            "product": {
               "type": "string",
               "description": "Продукт, который охватывает урок (например, Azure, Power BI и т. д.)"
            },
            "level": {
               "type": "string",
               "description": "Уровень опыта учащегося перед прохождением курса (например, начинающий, средний, продвинутый)"
            }
         },
         "required": [
            "role"
         ]
      }
   }
]
```

Давайте подробнее опишем каждый экземпляр функции ниже:

- `name` - Название функции, которую мы хотим вызвать.
- `description` - Это описание того, как функция работает. Здесь важно быть конкретным и ясным.
- `parameters` - Список значений и формата, которые вы хотите, чтобы модель использовала в своем ответе. Массив параметров состоит из элементов, у которых есть следующие свойства:
  1. `type` - Тип данных, в котором будут храниться свойства.
  2. `properties` - Список конкретных значений, которые модель будет использовать в своем ответе.
     1. `name` - Ключ - это имя свойства, которое модель будет использовать в своем отформатированном ответе, например, `product`.
     2. `type` - Тип данных этого свойства, например, `string`.
     3. `description` - Описание конкретного свойства.

Также есть необязательное свойство `required` - обязательное свойство для завершения функционального вызова.

### Шаг 3 - Выполнение функционального вызова

После определения функции мы теперь должны включить ее в вызов API Chat Completions. Мы делаем это, добавляя `functions` в запрос. В данном случае `functions=functions`.

Также есть возможность установить `function_call` в значение `auto`. Это означает, что мы позволим LLM самостоятельно решить, какую функцию вызвать на основе сообщения пользователя, а не назначать ее сами.

Вот пример кода, в котором мы вызываем `ChatCompletion.create`. Обратите внимание, как мы устанавливаем `functions=functions` и `function_call="auto"`, предоставляя LLM возможность самому решить, когда вызывать функции, которые мы предоставляем:

```python
response = client.chat.completions.create(model=deployment,
                                        messages=messages,
                                        functions=functions,
                                        function_call="auto")

print(response.choices[0].message)
```

Ответ, который мы получаем, выглядит следующим образом:

```json
{
  "role": "assistant",
  "function_call": {
    "name": "search_courses",
    "arguments": "{\n  \"role\": \"student\",\n  \"product\": \"Azure\",\n  \"level\": \"beginner\"\n}"
  }
}
```

Здесь мы можем увидеть, как функция `search_courses` была вызвана и с какими аргументами, перечисленными в свойстве `arguments` в JSON-ответе.

LLM смог найти данные, соответствующие аргументам функции, извлекая их из значения, предоставленного параметру `messages` в вызове `chat.completions`. Ниже приведено напоминание значения `messages`:

```python
messages = [{"role": "user", "content": "Найдите мне хороший курс для начинающего студента по изучению Azure."}]
```

Как видите, `student`, `Azure` и `beginner` были извлечены из `messages` и использованы в качестве входных данных для функции. Использование функций таким образом - отличный способ извлечь информацию из запроса, а также предоставить структуру LLM и иметь возможность повторного использования функциональности.

Далее нам нужно разобраться, как мы можем использовать это в нашем приложении.

## Интеграция вызовов функций в приложение

После того, как мы проверили отформатированный ответ от LLM, мы можем интегрировать его в наше приложение.

### Управление потоком

Для интеграции этого в наше приложение выполним следующие шаги:

1. Сначала давайте сделаем вызов к сервисам Open AI и сохраняем сообщение в переменной с именем `response_message`.

   ```python
   response_message = response.choices[0].message
   ```

2. Теперь мы определим функцию, которая будет вызывать API Microsoft Learn для получения списка курсов:

   ```python
   import requests

   def search_courses(role, product, level):
     url = "https://learn.microsoft.com/api/catalog/"
     params = {
        "role": role,
        "product": product,
        "level": level
     }
     response = requests.get(url, params=params)
     modules = response.json()["modules"]
     results = []
     for module in modules[:5]:
        title = module["title"]
        url = module["url"]
        results.append({"title": title, "url": url})
     return str(results)
   ```

   Обратите внимание, как мы теперь создаем настоящую функцию на языке Python, которая соответствует именам функций, указанным в переменной `functions`. Мы также делаем реальные внешние вызовы API для получения необходимых данных. В данном случае мы используем API Microsoft Learn для поиска учебных модулей.

Хорошо, мы создали переменные `functions` и соответствующую функцию на языке Python. Как мы сообщаем LLM, как связать их вместе, чтобы наша функция на языке Python была вызвана?

3. Чтобы узнать, нужно ли вызывать функцию на языке Python, нам необходимо изучить ответ LLM и узнать, является ли `function_call` его частью, а затем вызвать указанную функцию. Вот как можно выполнить указанную проверку:

   ```python
   # Проверяем, хочет ли модель вызвать функцию
   if response_message.function_call.name:
    print("Рекомендуемый вызов функции:")
    print(response_message.function_call.name)
    print()

    # Вызываем функцию.
    function_name = response_message.function_call.name

    available_functions = {
            "search_courses": search_courses,
    }
    function_to_call = available_functions[function_name]

    function_args = json.loads(response_message.function_call.arguments)
    function_response = function_to_call(**function_args)

    print("Результат вызова функции:")
    print(function_response)
    print(type(function_response))


    # Добавляем ответ ассистента и результат функции в сообщения
    messages.append( # добавление ответа ассистента в сообщения
        {
            "role": response_message.role,
            "function_call": {
                "name": function_name,
                "arguments": response_message.function_call.arguments,
            },
            "content": None
        }
    )
    messages.append( # добавление результата функции в сообщения
        {
            "role": "function",
            "name": function_name,
            "content":function_response,
        }
    )
   ```

   Эти три строки обеспечивают извлечение имени функции, аргументов и вызов функции:

   ```python
   function_to_call = available_functions[function_name]

   function_args = json.loads(response_message.function_call.arguments)
   function_response = function_to_call(**function_args)
   ```

   Ниже приведен вывод при выполнении нашего кода:

   **Вывод**

   ```Recommended Function call:
   {
     "name": "search_courses",
     "arguments": "{\n  \"role\": \"student\",\n  \"product\": \"Azure\",\n  \"level\": \"beginner\"\n}"
   }

   Output of function call:
   [{'title': 'Describe concepts of cryptography', 'url': 'https://learn.microsoft.com/training/modules/describe-concepts-of-cryptography/?
   WT.mc_id=api_CatalogApi'}, {'title': 'Introduction to audio classification with TensorFlow', 'url': 'https://learn.microsoft.com/en-
   us/training/modules/intro-audio-classification-tensorflow/?WT.mc_id=api_CatalogApi'}, {'title': 'Design a Performant Data Model in Azure SQL
   Database with Azure Data Studio', 'url': 'https://learn.microsoft.com/training/modules/design-a-data-model-with-ads/?
   WT.mc_id=api_CatalogApi'}, {'title': 'Getting started with the Microsoft Cloud Adoption Framework for Azure', 'url':
   'https://learn.microsoft.com/training/modules/cloud-adoption-framework-getting-started/?WT.mc_id=api_CatalogApi'}, {'title': 'Set up the
   Rust development environment', 'url': 'https://learn.microsoft.com/training/modules/rust-set-up-environment/?WT.mc_id=api_CatalogApi'}]
   <class 'str'>
   ```

4. Теперь мы отправим обновленное сообщение `messages` в LLM, чтобы получить естественный языковый ответ вместо ответа в формате API JSON.

   ```python
   print("Сообщения в следующем запросе:")
   print(messages)
   print()

   second_response = client.chat.completions.create(
      messages=messages,
      model=deployment,
      function_call="auto",
      functions=functions,
      temperature=0
   )  # получаем новый ответ от GPT, где он видит результат функции

   print(second_response.choices[0].message)
   ```

   **Вывод**

   ```python
   {
     "role": "assistant",
     "content": "Я нашел несколько хороших курсов для начинающих студентов, чтобы изучить Azure:\n\n1. [Описание концепций криптографии] (https://learn.microsoft.com/training/modules/describe-concepts-of-cryptography/?WT.mc_id=api_CatalogApi)\n2. [Введение в классификацию аудио с помощью TensorFlow](https://learn.microsoft.com/training/modules/intro-audio-classification-tensorflow/?WT.mc_id=api_CatalogApi)\n3. [Создание производительной модели данных в Azure SQL Database с использованием Azure Data Studio](https://learn.microsoft.com/training/modules/design-a-data-model-with-ads/?WT.mc_id=api_CatalogApi)\n4. [Начало работы с Microsoft Cloud Adoption Framework для Azure](https://learn.microsoft.com/training/modules/cloud-adoption-framework-getting-started/?WT.mc_id=api_CatalogApi)\n5. [Настройка среды разработки на Rust](https://learn.microsoft.com/training/modules/rust-set-up-environment/?WT.mc_id=api_CatalogApi)\n\nВы можете нажимать на ссылки, чтобы получить доступ к курсам."
   }

   ```

## Задание

Для продолжения изучения функционала вызова функций в Azure OpenAI вы можете выполнить следующее:

- Добавьте больше параметров функции, которые могут помочь учащимся найти больше курсов.
- Создайте другой вызов функции, который принимает больше информации от учащегося, например, его родной язык.
- Создайте обработку ошибок, когда вызов функции и/или вызов API не возвращает подходящих курсов.

Подсказка: Перейдите на страницу [Документация по API каталога Learn](https://learn.microsoft.com/training/support/catalog-api-developer-reference?WT.mc_id=academic-105485-koreyst), чтобы узнать, как и где доступны эти данные.

## Отличная работа! Продолжайте свое путешествие

После завершения этого урока ознакомьтесь с нашей [коллекцией обучения по генеративному искусственному интеллекту](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), чтобы продолжить развивать свои знания в области генеративного искусственного интеллекта!

Перейдите к Уроку 12, где мы рассмотрим, как [проектировать пользовательский интерфейс для приложений на основе искусственного интеллекта](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)!