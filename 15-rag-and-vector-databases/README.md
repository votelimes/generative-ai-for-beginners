# Retrieval Augmented Generation (RAG) и векторные базы данных

<!-- ![chapter image](./images/) -->

В уроке по поисковым приложениям мы кратко узнали, как интегрировать собственные данные в большие языковые модели (LLM). В этом уроке мы более подробно рассмотрим концепцию закрепления ваших данных в вашем приложении на основе LLM, механику этого процесса и методы хранения данных, включая векторные представления и текст.

> **Видео скоро появится**

## Введение

В этом уроке мы рассмотрим следующие вопросы:

- Введение в RAG, что это такое и почему это используется в искусственном интеллекте (ИИ).

- Понимание, что такое векторные базы данных и создание одной для нашего приложения.

- Практический пример интеграции RAG в приложение.

## Цели обучения

После завершения этого урока вы сможете:

- Объяснить значение RAG в поиске и обработке данных.

- Настроить приложение RAG и закрепить ваши данные в LLM.

- Эффективно интегрировать RAG и векторные базы данных в приложениях LLM.

## Наш сценарий: улучшение LLM с помощью собственных данных

В этом уроке мы хотим добавить свои собственные заметки в образовательный стартап, чтобы чат-бот мог получать больше информации по разным предметам. Используя имеющиеся заметки, учащиеся смогут лучше изучать материал и понимать различные темы, что облегчит подготовку к экзаменам. Для создания нашего сценария мы будем использовать следующие компоненты:

- `Azure OpenAI`: LLM, который мы будем использовать для создания нашего чат-бота.

- `Урок "Искусственные нейронные сети для начинающих"`: это будут данные, на которых мы закрепим наш LLM.

- `Azure AI Search` и `Azure Cosmos DB`: векторная база данных для хранения наших данных и создания поискового индекса.

Пользователи смогут создавать практические викторины на основе своих заметок, карточки для повторения и получать краткие обзоры материала. Чтобы начать, давайте рассмотрим, что такое RAG и как оно работает:

## Retrieval Augmented Generation (RAG)

Восстановление сгенерированной информации (Retrieval Augmented Generation, RAG) - это метод, при котором чат-бот на основе модели глубокого обучения обрабатывает запросы пользователей и генерирует ответы. Он разработан для интерактивного общения с пользователями по широкому спектру тем. Однако его ответы ограничены предоставленным контекстом и фундаментальными данными, на которых он был обучен. Например, модель GPT-4 имеет ограничение по знаниям, полученным к сентябрю 2021 года, что означает, что у нее нет информации о событиях, произошедших после этого периода. Кроме того, данные, используемые для обучения моделей глубокого обучения, исключают конфиденциальную информацию, такую как личные заметки или руководство по продукту компании.

### Как работают модели RAG (Retrieval Augmented Generation)

![схема, показывающая работу моделей RAG](images/how-rag-works.png?WT.mc_id=academic-105485-koreyst)

Предположим, вы хотите развернуть чат-бота, который создает викторины на основе ваших заметок. Для этого вам потребуется подключение к базе знаний. И здесь на помощь приходит RAG. Модели RAG работают следующим образом:

- **База знаний:** Перед извлечением информации эти документы должны быть обработаны и предварительно подготовлены. Обычно они разбиваются на более мелкие части, преобразуются в текстовое представление и сохраняются в базе данных.

- **Запрос пользователя:** Пользователь задает вопрос.

- **Извлечение информации:** Когда пользователь задает вопрос, модель сопоставления извлекает соответствующую информацию из базы знаний, чтобы предоставить дополнительный контекст, который будет включен в запрос.

- **Улучшение генерации:** Модель LLM улучшает свой ответ на основе полученных данных. Это позволяет сгенерированному ответу не только основываться на предварительно обученных данных, но также на актуальной информации из добавленного контекста. Извлеченные данные используются для улучшения ответов модели LLM. Затем модель LLM возвращает ответ на вопрос пользователя.

![рисунок, показывающий архитектуру RAG](images/encoder-decode.png?WT.mc_id=academic-105485-koreyst)

Архитектура моделей RAG реализована с использованием трансформеров, состоящих из двух частей: энкодера и декодера. Например, когда пользователь задает вопрос, входной текст "кодируется" в векторы, которые захватывают смысл слов, а затем эти векторы "декодируются" в наш индекс документов и генерируют новый текст на основе запроса пользователя. Модель LLM использует энкодер-декодер для генерации выходных данных.

Два подхода при внедрении RAG согласно предлагаемому документу: [Поисково-дополненная генерация для наукоемких задач НЛП (программного обеспечения обработки естественного языка)](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst) are:

- **RAG-Sequence** использует извлеченные документы для предсказания наилучшего возможного ответа на запрос пользователя.

- **RAG-Token** использует документы для генерации следующего токена, а затем извлекает их, чтобы ответить на вопрос пользователя.

### Почему использовать модели RAG?

- **Информационная насыщенность:** обеспечивает актуальность и актуальность текстовых ответов. Таким образом, модели RAG повышают производительность в задачах, связанных с конкретной областью, путем доступа к внутренней базе знаний.

- Снижает создание фальсификаций, используя **проверяемые данные** в базе знаний для предоставления контекста для запросов пользователей.

- Это **экономически эффективно**, поскольку модели RAG более экономичны по сравнению с тонкой настройкой моделей LLM.

## Создание базы знаний

Наше приложение основано на наших собственных данных, а именно, на уроке по нейронным сетям в программе "Искусственный интеллект для начинающих".

### Векторные базы данных

Векторная база данных, в отличие от традиционных баз данных, является специализированной базой данных, разработанной для хранения, управления и поиска векторных представлений. Она хранит числовые представления документов. Разбиение данных на числовые вложения облегчает понимание и обработку данных нашей системой искусственного интеллекта.

Мы храним наши вложения в векторных базах данных, поскольку модели LLM имеют ограничение на количество токенов, которые они могут принимать на вход. Так как невозможно передать все вложения в модель LLM, нам необходимо разбить их на части, и когда пользователь задает вопрос, вложения, наиболее подходящие для этого вопроса, будут возвращены вместе с запросом. Разделение также снижает расходы на количество токенов, проходящих через модель LLM.

Некоторые популярные векторные базы данных включают Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant и DeepLake. Вы можете создать модель Azure Cosmos DB с помощью Azure CLI с помощью следующей команды:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### От текста к вложениям

Прежде чем мы сохраняем наши данные, нам нужно преобразовать их в векторные вложения, которые будут храниться в базе данных. Если вы работаете с большими документами или длинными текстами, вы можете разделить их на фрагменты в соответствии с ожидаемыми запросами. Разделение можно выполнить на уровне предложений или на уровне абзацев. Поскольку разделение извлекает значения из слов вокруг них, вы можете добавить к фрагменту некоторый другой контекст, например, добавив заголовок документа или включив некоторый текст перед или после фрагмента. Вы можете выполнить разделение данных следующим образом:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

   # Если последний фрагмент не достиг минимальной длины, все равно добавьте его
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

После разделения данных на фрагменты мы можем преобразовать наш текст с использованием различных моделей вложений. Некоторые модели, которые вы можете использовать, включают word2vec, ada-002 от OpenAI, Azure Computer Vision и многие другие. Выбор модели зависит от используемых языков, типа закодированного контента (текст/изображения/аудио), размера входных данных, которые она может обработать, и длины вывода вложения.

Пример вложенного текста с использованием модели `text-embedding-ada-002` от OpenAI:
![вложение слова "cat"](images/cat.png?WT.mc_id=academic-105485-koreyst)

## Извлечение и поиск по векторам

Когда пользователь задает вопрос, извлекатель преобразует его в вектор с помощью кодировщика запроса, затем ищет в нашем индексе поиска документов соответствующие векторы, связанные с входными данными. После этого он преобразует как входной вектор, так и векторы документов в текст и передает их модели LLM.

### Извлечение

Извлечение происходит, когда система пытается быстро найти документы в индексе, удовлетворяющие критериям поиска. Целью извлекателя является получение документов, которые будут использоваться для предоставления контекста и базового знания модели LLM на основе ваших данных.

Существует несколько способов выполнения поиска в нашей базе данных:

- **Поиск по ключевым словам** - используется для текстовых запросов.

- **Семантический поиск** - использует семантическое значение слов.

- **Поиск по векторам** - преобразует документы из текста в векторные представления с использованием моделей вложений. Извлечение будет осуществляться путем запроса документов, векторные представления которых наиболее близки к вопросу пользователя.

- **Гибридный поиск** - комбинация поиска по ключевым словам и поиска по векторам.

Одной из проблем с извлечением является отсутствие подходящего ответа на запрос в базе данных. В этом случае система вернет наилучшую информацию, которую сможет получить. Однако вы можете использовать тактики, такие как установка максимального расстояния для определения релевантности или использование гибридного поиска, который объединяет как поиск по ключевым словам, так и поиск по векторам. В этом уроке мы будем использовать гибридный поиск, комбинируя поиск по векторам и по ключевым словам. Мы будем хранить наши данные в таблице с колонками, содержащими фрагменты и вложения.

### Сходство векторов

Извлекатель будет искать в базе знаний вложения, которые находятся близко друг к другу, как ближайшие соседи, поскольку это тексты, которые похожи между собой. В сценарии, когда пользователь задает запрос, он сначала преобразуется в вложение, а затем сопоставляется с похожими вложениями. Обычно для определения степени сходства между различными векторами используется косинусная схожесть, основанная на угле между двумя векторами.

Мы также можем измерять сходство с помощью других альтернативных методов, таких как евклидово расстояние, которое представляет собой прямую линию между конечными точками векторов, и скалярное произведение, которое измеряет сумму произведений соответствующих элементов двух векторов.

### Индекс поиска

При выполнении извлечения нам потребуется создать индекс поиска для нашей базы знаний перед выполнением поиска. Индекс будет хранить наши вложения и сможет быстро извлекать наиболее похожие фрагменты, даже в большой базе данных. Мы можем создать наш индекс локально, используя следующий код на языке Python:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Создаем индекс поиска
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# Для запроса к индексу можно использовать метод kneighbors
distances, indices = nbrs.kneighbors(embeddings)
```

### Переупорядочивание

После выполнения запроса к базе данных может потребоваться отсортировать результаты по наиболее релевантным. Модель переупорядочивания LLM использует методы машинного обучения для улучшения релевантности результатов поиска путем их упорядочивания от наиболее релевантных. Если вы используете Azure AI Search, переупорядочивание будет выполняться автоматически с помощью семантического переупорядочивателя. Вот пример того, как работает переупорядочивание с использованием ближайших соседей:

```python
# Находим наиболее похожие документы
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Выводим наиболее похожие документы
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"Индекс {index} не найден в DataFrame")
```

## Объединение всех элементов вместе

Последний шаг - добавление нашей LLM в процесс, чтобы получать ответы, основанные на наших данных. Мы можем реализовать это следующим образом:

```python
user_input = "что такое перцептрон?"

def chatbot(user_input):
    # Преобразование вопроса в вектор запроса
    query_vector = create_embeddings(user_input)

    # Находим наиболее похожие документы
    distances, indices = nbrs.kneighbors([query_vector])

    # Добавляем документы в запрос для предоставления контекста
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # Объединяем историю и ввод пользователя
    history.append(user_input)

    # Создаем объект сообщения
    messages=[
        {"role": "system", "content": "Вы - виртуальный помощник, который помогает с вопросами по искусственному интеллекту."},
        {"role": "user", "content": history[-1]}
    ]

    # Используем завершение чата для генерации ответа
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
```

## Оценка нашего приложения

### Метрики оценки

- Качество предоставляемых ответов, обеспечивая их естественность, свободное и гармоничное звучание, а также схожесть с человеческими высказываниями.

- Прикрепление к данным: оценка того, насколько ответ основан на предоставленных документах.

- Релевантность: оценка соответствия ответа и связи с заданным вопросом.

- Грамматическая правильность: проверка того, имеет ли ответ грамматический смысл.

## Сферы применения использования RAG (Retrieval-Augmented Generation) и векторных баз данных

Существует множество различных сфер применения, где функции вызовов могут улучшить ваше приложение, например:

- Вопросно-ответные системы: закрепление информации о компании в чате, который может использоваться сотрудниками для задавания вопросов.

- Системы рекомендаций: создание системы, которая находит наиболее схожие значения, например, фильмы, рестораны и многое другое.

- Сервисы чатботов: хранение истории чата и персонализация беседы на основе данных пользователя.

- Поиск изображений на основе векторных вложений, полезный при выполнении распознавания изображений и обнаружении аномалий.

## Заключение

Мы рассмотрели основные аспекты RAG - от добавления данных в приложение до запросов пользователя и вывода результатов. Чтобы упростить создание RAG, вы можете использовать фреймворки, такие как Semanti Kernel, Langchain или Autogen.

## Задание

Для продолжения изучения Retrieval Augmented Generation (RAG) вы можете выполнить следующие задачи:

- Создать пользовательский интерфейс (фронтенд) для приложения, используя выбранный вами фреймворк.

- Использовать фреймворк LangChain или Semantic Kernel и воссоздать свое приложение.

Поздравляю с завершением урока! 👏

## Обучение не останавливается здесь, продолжайте свое путешествие

После завершения этого урока ознакомьтесь с нашей [коллекцией обучающих материалов по генеративному искусственному интеллекту](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), чтобы продолжить расширение своих знаний в области генеративного ИИ!
