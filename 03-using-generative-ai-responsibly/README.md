# Ответственное использование генеративного ИИ (принципы Responsible AI)

[![Ответственное использование генеративного ИИ](./images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)]() 

> **Видео скоро появится**

Легко увлечься искусственным интеллектом и, в частности, генеративным искусственным интеллектом, но вам нужно подумать о том, как вы будете использовать его ответственно. Вам необходимо подумать о том, как обеспечить честность, безвредность результатов и многое другое. Цель этой главы — предоставить вам упомянутый контекст, узнать, что следует учитывать и как предпринять активные шаги для улучшения использования ИИ.

## Введение

В этом уроке будут рассмотрены:

- Почему вам следует использовать Responsible AI при создании приложений генеративного ИИ.
- Основные принципы Responsible AI и их отношение к генеративному ИИ.
- Как реализовать принципы Responsible AI на практике с помощью стратегии и инструментов.

## Цели обучения

Пройдя этот урок, вы будете знать:

- Важность Responsible AI при создании приложений генеративного ИИ.
- Когда следует подумать и применить основные принципы Responsible AI при создании приложений генеративного ИИ.
- Какие инструменты и стратегии вам доступны для реализации концепции Responsible AI на практике.

## Responsible AI принципы

Волнение от генеративного искусственного интеллекта никогда не было таким высоким. Это волнение привлекло в отрасль множество новых разработчиков, внимание и финансирование. Хотя это очень позитивно для всех, кто хочет создавать продукты и компании с использованием генеративного искусственного интеллекта, также важно, чтобы мы действовали ответственно.

На протяжении всего этого курса мы концентрируемся на создании нашего стартапа и образовательного продукта в области искусственного интеллекта. Мы будем использовать принципы Responsible AI: справедливость, инклюзивность, надежность/безопасность, безопасность и конфиденциальность, прозрачность и подотчетность. Используя эти принципы, мы исследуем, как они связаны с использованием генеративного искусственного интеллекта в наших продуктах..

## Почему вам следует отдавать приоритет ответственному ИИ

При создании продукта использование человекоориентированного подхода, учитывающего интересы пользователя, приводит к лучшим результатам.

Уникальность генеративного искусственного интеллекта заключается в его способности создавать полезные ответы, информацию, рекомендации и контент для пользователей. Это можно сделать без большого количества ручных действий, что может привести к очень впечатляющим результатам. Без надлежащего планирования и стратегии это, к сожалению, может привести к некоторым вредным последствиям для ваших пользователей, вашего продукта и общества в целом.

Давайте посмотрим на некоторые (но не на все) из этих потенциально вредных результатов:

### Галлюцинации

Галлюцинации — это термин, используемый для описания случаев, когда LLM создает контент, который либо совершенно бессмысленен, либо что-то, что, как мы знаем, фактически неверно на основании других источников информации.

Давайте, например, создадим функцию для нашего стартапа, которая позволяет студентам задавать модели исторические вопросы. Студент задает вопрос: «Кто был единственным выжившим на Титанике?»

Модель выдает ответ, подобный приведенному ниже:

![Подсказка: «Кто был единственным выжившим на Титанике?»](../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> *(Источник: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))*

Единственным выжившим после катастрофы «Титаника» был Чарльз Джоуин. Он был британским пекарем и главным пекарем на борту «Титаника». Джогину удалось выжить при затоплении, как сообщается, цепляясь за перевернутую спасательную шлюпку и проведя несколько часов в холодной воде, прежде чем его спасло судно RMS Carpathia. Его выживание часто связывают с чрезмерным употреблением алкоголя, который, возможно, помог ему пережить сильный холод. Джоуин дожил до 1956 года и позже предоставил свидетельства о своем опыте во время катастрофы «Титаника».

Это очень уверенный и обстоятельный ответ. К сожалению, это неверно. Даже при минимальном объеме исследований можно было бы обнаружить, что после катастрофы «Титаника» выжило несколько человек. Для студента, который только начинает исследовать эту тему, этот ответ может быть достаточно убедительным, чтобы его не подвергали сомнению и не воспринимали как факт. Последствия этого могут привести к тому, что система искусственного интеллекта станет ненадежной и негативно отразится на репутации нашего стартапа.

С каждой итерацией любого LLM мы наблюдали улучшение производительности в плане минимизации галлюцинаций. Даже несмотря на это улучшение, нам, разработчикам приложений и пользователям, по-прежнему необходимо помнить об этих ограничениях.

### Вредный контент

В предыдущем разделе мы рассмотрели случаи, когда LLM выдает неправильные или бессмысленные ответы. Еще один риск, о котором нам нужно знать, — это когда модель отвечает вредоносным контентом.

Вредный контент можно определить как:

- Предоставление инструкций или поощрение членовредительства или причинения вреда определенным группам.
- Ненавистнический или унизительный контент.
- Руководство планированием любого типа нападения или насильственных действий.
- Предоставление инструкций о том, как найти незаконный контент или совершить незаконные действия.
- Демонстрация контента откровенно сексуального характера.

Что касается нашего стартапа, мы хотим убедиться, что у нас есть правильные инструменты и стратегии, чтобы студенты не видели этот тип контента.

### Отсутствие справедливости

Справедливость определяется как «гарантия того, что система ИИ свободна от предвзятости и дискриминации и что они относятся ко всем справедливо и одинаково». В мире генеративного искусственного интеллекта мы хотим гарантировать, что исключительные мировоззрения маргинализированных групп не подкрепляются результатами модели.

Подобные результаты не только разрушительны для формирования положительного опыта использования продукта у наших пользователей, но и наносят дополнительный вред обществу. Как разработчики приложений, мы всегда должны учитывать широкую и разнообразную базу пользователей при создании решений с использованием генеративного ИИ.

## Как ответственно использовать генеративный искусственный интеллект

Теперь, когда мы определили важность принципов Responsible AI, давайте рассмотрим 4 шага, которые мы можем предпринять для ответственного создания наших решений ИИ:

![Смягчение цикла](./images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### Оценка потенциального вреда

При тестировании программного обеспечения мы проверяем ожидаемые действия пользователя в приложении. Аналогичным образом, тестирование разнообразного набора подсказок, которые пользователи, скорее всего, будут использовать, является хорошим способом измерения потенциального вреда.

Поскольку наш стартап создает образовательный продукт, было бы неплохо подготовить список подсказок, связанных с образованием. Это может быть освещение определенной темы, исторических фактов и подсказок о студенческой жизни.

### Смягчение потенциального вреда

Настало время найти способы предотвратить или ограничить потенциальный вред, причиняемый моделью и ее реакциями. Мы можем посмотреть на это в 4 разных слоях:

![Уровни смягчения последствий](./images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **Модель**. Выбор подходящей модели для конкретного случая использования. Более крупные и сложные модели, такие как GPT-4, могут вызвать больший риск вредоносного контента при применении к меньшим и более конкретным случаям использования. Использование данных обучения для точной настройки также снижает риск появления вредоносного контента.

- **Система безопасности**. Система безопасности — это набор инструментов и конфигураций на платформе, обслуживающих модель, которые помогают снизить ущерб. Примером этого является система фильтрации контента в службе Azure OpenAI. Системы также должны обнаруживать атаки с помощью джейлбрейка и нежелательную активность, например запросы от ботов.

- **Метаподсказка**. Метаподсказки и заземление — это способы, с помощью которых мы можем направлять или ограничивать модель на основе определенного поведения и информации. Это может быть использование входных данных системы для определения определенных ограничений модели. Кроме того, предоставление результатов, более соответствующих масштабу или области деятельности системы.

 Также можно использовать такие методы, как поисковая дополненная генерация (RAG), чтобы модель извлекала информацию только из выбранных надежных источников. Далее в этом курсе есть урок по [созданию поисковых приложений](../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Пользовательский опыт**. На последнем уровне пользователь каким-то образом напрямую взаимодействует с моделью через интерфейс нашего приложения. Таким образом, мы можем спроектировать UI/UX так, чтобы ограничить пользователя типами входных данных, которые он может отправлять в модель, а также текстом или изображениями, отображаемыми пользователю. При развертывании приложения ИИ мы также должны четко понимать, что может и чего не может делать наше приложение генеративного ИИ.  

У нас есть целый урок, посвященный [проектированию UX для приложений AI](../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)

- **Оценить модель**. Работа с LLM может быть сложной задачей, поскольку мы не всегда можем контролировать данные, на которых обучалась модель. В любом случае, мы всегда должны оценивать производительность и результаты модели. По-прежнему важно измерять точность модели, сходство, обоснованность и релевантность результатов. Это помогает обеспечить прозрачность и доверие к заинтересованным сторонам и пользователям.

### Используйте Responsible Generative AI решения

Создание операционной практики вокруг ваших приложений ИИ — это заключительный этап. Это включает в себя партнерство с другими подразделениями нашего стартапа, такими как юридический отдел и отдел безопасности, чтобы обеспечить соблюдение всех нормативных политик. Перед запуском мы также хотим составить планы доставки, обработки инцидентов и отката, чтобы предотвратить рост любого вреда для наших пользователей.

## Инструменты

Хотя работа по разработке Responsible Generative AI может показаться трудоемкой, она того стоит. По мере роста области генеративного искусственного интеллекта будет появляться больше инструментов, которые помогут разработчикам эффективно интегрировать ответственность в свои рабочие процессы. Например, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst ) может помочь обнаружить вредоносный контент и изображения через запрос API.

## Проверка знаний

О чем вам нужно позаботиться, чтобы обеспечить ответственное использование ИИ?

1. О том, что ответ правильный.
2. Вредное использование: ИИ не используется в преступных целях.
3. Обеспечение свободы ИИ от предвзятости и дискриминации.

Ответ: 2 и 3 верны. Ответственный ИИ помогает вам подумать о том, как смягчить вредные последствия и предвзятости и многое другое.

## 🚀 Испытание

Следите за публикациями [Azure AI Content Saftey](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst) и посмотрите, что вы можете использовать в своих целях.

## Отличная работа, продолжайте обучение

После завершения этого урока ознакомьтесь с нашим [Generative AI Learning collection](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) чтобы продолжить совершенствовать свои знания о генеративном искусственном интеллекте!

Переходим к уроку 4, где мы рассмотрим [основы быстрого проектирования](../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)!
