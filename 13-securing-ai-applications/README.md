Защита ваших приложений генеративного искусственного интеллекта

[![Защита ваших приложений генеративного искусственного интеллекта](./images/13-lesson-banner.jpg?WT.mc_id=academic-105485-koreyst)]()

## Введение

В этом уроке будет рассмотрено:

- Безопасность в контексте систем искусственного интеллекта.
- Распространенные риски и угрозы для систем искусственного интеллекта.
- Методы и соображения по обеспечению безопасности систем искусственного интеллекта.

## Цели обучения

После завершения этого урока вы будете понимать:

- Угрозы и риски для систем искусственного интеллекта.
- Распространенные методы и практики по обеспечению безопасности систем искусственного интеллекта.
- Как реализация тестирования безопасности может предотвратить неожиданные результаты и подорвать доверие пользователей.

## Что означает безопасность в контексте генеративного искусственного интеллекта?

Поскольку технологии искусственного интеллекта (ИИ) и машинного обучения (МО) все больше влияют на нашу жизнь, крайне важно защищать не только данные клиентов, но и сами системы искусственного интеллекта. ИИ/МО все чаще используются в поддержку процессов принятия высокостоимостных решений в отраслях, где неправильное решение может привести к серьезным последствиям.

Вот основные моменты, которые следует учесть:

- **Влияние ИИ/МО**: ИИ/МО имеют значительное влияние на повседневную жизнь, и поэтому обеспечение их безопасности становится неотъемлемой необходимостью.
- **Проблемы безопасности**: Это влияние ИИ/МО требует должного внимания, чтобы решить необходимость защиты ИИ-продуктов от сложных атак, будь то со стороны троллей или организованных групп.
- **Стратегические проблемы**: Техническая отрасль должна активно решать стратегические проблемы, чтобы обеспечить безопасность клиентов и защиту данных в долгосрочной перспективе.

Кроме того, модели машинного обучения в значительной степени не могут различать злонамеренные данные и безвредные аномальные данные. Основной источник данных для обучения получается из некурированных, немодерированных публичных наборов данных, к которым открыт доступ для внесения вклада сторонними лицами. Злоумышленникам не нужно компрометировать наборы данных, когда они могут свободно вносить свой вклад в них. С течением времени недостоверные злонамеренные данные становятся доверенными данными высокой достоверности, если структура/формат данных остается правильным.

Поэтому крайне важно обеспечить целостность и защиту хранилищ данных, которые используют ваши модели для принятия решений.

## Понимание угроз и рисков искусственного интеллекта

В контексте искусственного интеллекта и связанных систем, наиболее значительной угрозой безопасности сегодня является отравление данных. Отравление данных происходит, когда кто-то намеренно изменяет информацию, используемую для обучения искусственного интеллекта, чтобы вызвать ошибки в его работе. Это связано с отсутствием стандартизированных методов обнаружения и смягчения последствий, а также с нашей зависимостью от ненадежных или некурированных публичных наборов данных для обучения. Чтобы сохранить целостность данных и предотвратить ошибочный процесс обучения, важно отслеживать происхождение и историю ваших данных. В противном случае, известное выражение "мусор на входе, мусор на выходе" оказывается верным, что приводит к компрометации производительности модели.

Вот примеры того, как отравление данных может повлиять на ваши модели:

1. **Переключение меток**: В задаче бинарной классификации злоумышленник намеренно меняет метки небольшой части тренировочных данных. Например, безвредные образцы помечаются как вредоносные, что приводит к изучению моделью неправильных ассоциаций.\
   **Пример**: Фильтр спама неправильно классифицирует легитимные электронные письма как спам из-за подделанных меток.

2. **Отравление признаков**: Злоумышленник незаметно изменяет признаки в тренировочных данных, чтобы внести предвзятость или ввести в заблуждение модель.\
   **Пример**: Добавление несвязанных ключевых слов в описания товаров для манипуляции с системами рекомендаций.

3. **Внедрение данных**: Внедрение вредоносных данных в тренировочный набор для влияния на поведение модели.\
   **Пример**: Введение фальшивых отзывов пользователей для искажения результатов анализа тональности.

4. **Атаки с использованием задней двери**: Злоумышленник вставляет скрытый образец (заднюю дверь) в тренировочные данные. Модель учится распознавать этот образец и ведет себя злонамеренно при его активации.\
   **Пример**: Система распознавания лиц, обученная с использованием изображений с задними дверями, ошибочно идентифицирует определенного человека.

Корпорация MITRE создала [ATLAS (Адверсариальный ландшафт угроз для систем искусственного интеллекта)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), базу знаний тактик и техник, используемых злоумышленниками при реальных атаках на системы искусственного интеллекта.

Существует все больше уязвимостей в системах, основанных на искусственном интеллекте, так как внедрение ИИ увеличивает поверхность атаки существующих систем за пределами традиционных кибератак. Мы разработали ATLAS для повышения осведомленности об этих уникальных и развивающихся уязвимостях, поскольку глобальное сообщество все больше внедряет ИИ в различные системы. ATLAS основана на фреймворке MITRE ATT&CK® и ее тактики, техники и процедуры (TTP) дополняют те, которые представлены в ATT&CK.

Подобно фреймворку MITRE ATT&CK®, который широко используется в традиционной кибербезопасности для планирования сценариев эмуляции продвинутых угроз, ATLAS предоставляет легко искомый набор TTP, который поможет лучше понять и подготовиться к защите от новых атак.

Кроме того, Open Web Application Security Project (OWASP) создал "[Список Топ-10](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)" наиболее критических уязвимостей, обнаруженных в приложениях, использующих LLM. В списке подчеркиваются риски таких угроз, как упомянутое ранее загрязнение данных, а также другие, например:

- **Внедрение команд**: техника, при которой злоумышленники манипулируют моделью искусственного интеллекта путем тщательно созданных входных данных, заставляя ее вести себя не по своему предназначению.
- **Уязвимости цепочки поставок**: компоненты и программное обеспечение, составляющие приложения, используемые LLM, такие как модули Python или внешние наборы данных, могут быть скомпрометированы, что приводит к неожиданным результатам, внедрению предубеждений и даже уязвимостям в основной инфраструктуре.
- **Слишком большая зависимость**: LLM не являются безошибочными и могут создавать галлюцинации, предоставляя неточные или небезопасные результаты. В нескольких задокументированных случаях люди принимали эти результаты на веру, что приводило к непреднамеренным отрицательным последствиям в реальном мире.

Род Трент, эксперт Microsoft Cloud Advocate, написал бесплатную книгу "[Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)", которая глубоко исследует эти и другие новые угрозы ИИ и предоставляет подробные рекомендации о том, как наилучшим образом справиться с такими сценариями.

## Тестирование безопасности для систем и LLM на основе искусственного интеллекта

Искусственный интеллект (ИИ) преобразует различные области и отрасли, предлагая новые возможности и преимущества для общества. Однако ИИ также представляет значительные вызовы и риски, такие как конфиденциальность данных, предвзятость, отсутствие объяснимости и возможное неправомерное использование. Поэтому крайне важно обеспечить безопасность и ответственность систем ИИ, то есть соблюдение этических и правовых стандартов и доверие со стороны пользователей и заинтересованных сторон.

Тестирование безопасности - это процесс оценки безопасности системы ИИ или LLM путем выявления и эксплуатации их уязвимостей. Это может выполняться разработчиками, пользователями или сторонними аудиторами в зависимости от цели и объема тестирования. Некоторые из наиболее распространенных методов тестирования безопасности для систем ИИ и LLM включают:

- **Очистка данных**: это процесс удаления или анонимизации конфиденциальной или личной информации из обучающих данных или входных данных системы ИИ или LLM. Очистка данных может помочь предотвратить утечку данных и злонамеренное вмешательство путем сокращения доступа к конфиденциальным или личным данным.
- **Адверсариальное тестирование**: это процесс создания и применения адверсариальных примеров к входным или выходным данным системы ИИ или LLM для оценки их устойчивости и устойчивости к адверсарным атакам. Адверсариальное тестирование может помочь выявить и устранить уязвимости и слабые места системы ИИ или LLM, которые могут быть использованы злоумышленниками.
- **Проверка модели**: это процесс проверки правильности и полноты параметров модели или архитектуры системы ИИ или LLM. Проверка модели может помочь обнаружить и предотвратить кражу модели, обеспечивая защиту и аутентификацию модели.
- **Проверка вывода**: это процесс проверки качества и надежности вывода системы ИИ или LLM. Проверка вывода может помочь обнаружить и исправить злонамеренное вмешательство, обеспечивая согласованность и точность вывода.

OpenAI, лидер в области систем искусственного интеллекта, создал серию _оценок безопасности_ в рамках своей инициативы по междисциплинарному тестированию, направленной на тестирование вывода систем ИИ в надежде на вклад в область безопасности ИИ.

> Оценки могут варьироваться от простых тестов вопросов и ответов до более сложных симуляций. Вот некоторые примеры оценок, разработанных OpenAI для оценки поведения ИИ с разных точек зрения:

#### Убеждение

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): Насколько хорошо система искусственного интеллекта может обмануть другую систему искусственного интеллекта, чтобы она произнесла секретное слово?
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): Насколько хорошо система искусственного интеллекта может убедить другую систему искусственного интеллекта пожертвовать деньги?
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): Насколько хорошо система искусственного интеллекта может повлиять на поддержку политического предложения другой системы искусственного интеллекта?

#### Стеганография (скрытое сообщение)

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): Насколько хорошо система искусственного интеллекта может передавать секретные сообщения, не попадаясь на глаза другой системе искусственного интеллекта?
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): Насколько хорошо система искусственного интеллекта может сжимать и распаковывать сообщения, чтобы скрывать секретные сообщения?
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): Насколько хорошо система искусственного интеллекта может координироваться с другой системой искусственного интеллекта без прямого общения?

### Безопасность искусственного интеллекта

Крайне важно стремиться защищать системы искусственного интеллекта от злонамеренных атак, неправомерного использования или непредвиденных последствий. Это включает принятие мер для обеспечения безопасности, надежности и надежности систем искусственного интеллекта, таких как:

- Защита данных и алгоритмов, используемых для обучения и функционирования моделей искусственного интеллекта.
- Предотвращение несанкционированного доступа, манипуляций или саботажа систем искусственного интеллекта.
- Обнаружение и смягчение предвзятости, дискриминации или этических проблем в системах искусственного интеллекта.
- Обеспечение ответственности, прозрачности и объяснимости решений и действий искусственного интеллекта.
- Выстраивание целей и ценностей систем искусственного интеллекта в соответствии с целями и ценностями людей и общества.

Безопасность искусственного интеллекта важна для обеспечения целостности, доступности и конфиденциальности систем искусственного интеллекта и данных. Некоторые из вызовов и возможностей в области безопасности искусственного интеллекта включают:

- Возможность: Внедрение искусственного интеллекта в стратегии кибербезопасности, поскольку он может сыграть важную роль в выявлении угроз и улучшении времени реагирования. Искусственный интеллект может помочь автоматизировать и усилить обнаружение и смягчение кибератак, таких как фишинг, вредоносные программы или программы-вымогатели.
- Вызов: Искусственный интеллект также может использоваться противниками для запуска сложных атак, таких как генерация поддельного или вводящего в заблуждение контента, подделка пользователей или эксплуатация уязвимостей в системах искусственного интеллекта. Поэтому разработчики искусственного интеллекта несут особую ответственность за создание систем, которые устойчивы и устойчивы к злоупотреблению.

### Защита данных

Системы искусственного интеллекта, такие как LLM, могут представлять угрозы для конфиденциальности и безопасности используемых ими данных. Например, LLM может потенциально запомнить и утечь чувствительную информацию из своих обучающих данных, такую как имена, адреса, пароли или номера кредитных карт. Он также может быть подвержен манипуляциям или атакам злонамеренных акторов, которые хотят использовать его уязвимости или предвзятости в своих интересах. Поэтому важно знать об этих рисках и принимать соответствующие меры для защиты данных, используемых с LLM. Существует несколько шагов, которые можно предпринять для защиты данных, используемых с LLM. Эти шаги включают:

- **Ограничение объема и типа данных, которые вы делитесь с LLM**: Делитесь только теми данными, которые необходимы и соответствующие задачам, и избегайте передачи любых данных, которые являются чувствительными, конфиденциальными или персональными. Пользователи также должны анонимизировать или шифровать данные, которые они передают LLM, например, путем удаления или маскировки любой идентифицирующей информации или использования защищенных каналов связи.
- **Проверка данных, генерируемых LLM**: Всегда проверяйте точность и качество результатов, создаваемых LLM, чтобы убедиться, что они не содержат нежелательной или неподходящей информации.
- **Сообщение о любых нарушениях данных или инцидентах**: Будьте бдительны в отношении любых подозрительных или необычных действий или поведения со стороны LLM, таких как создание текстов, которые несут никакой ценности, неточности, оскорбительности или вреда. Это может свидетельствовать о нарушении данных или инциденте безопасности.

Безопасность данных, управление и соответствие требованиям являются критически важными для любой организации, которая хочет использовать мощь данных и искусственного интеллекта в многоклаудовой среде. Обеспечение безопасности и управление всеми вашими данными являются сложными и многоаспектными задачами. Вам необходимо обеспечить безопасность и управлять различными типами данных (структурированными, неструктурированными и данными, создаваемыми искусственным интеллектом) в различных местоположениях на нескольких облачных платформах, а также учесть существующие и будущие требования безопасности данных, управления искусственным интеллектом и соответствия правилам. Чтобы защитить ваши данные, вам необходимо применять некоторые bewt-практики и предосторожности, такие как:

- Используйте облачные сервисы или платформы, предлагающие функции защиты данных и конфиденциальности.
- Используйте инструменты для контроля качества данных и их проверки на наличие ошибок, несоответствий или аномалий.
- Используйте фреймворки управления данными и этики, чтобы обеспечить ответственное и прозрачное использование ваших данных.

### Эмуляция угроз реального мира - AI red teaming

Эмуляция угроз реального мира сегодня является стандартной практикой при создании устойчивых AI систем путем использования аналогичных инструментов, тактик и процедур для выявления рисков для систем и проверки реакции защитников.

> Практика AI red teaming развивается и приобретает более широкое значение: она не только охватывает проверку на наличие уязвимостей безопасности, но также включает проверку на другие сбои системы, такие как генерация потенциально вредоносного контента. AI системы представляют новые риски, и red teaming является ключевым элементом понимания этих новых рисков, таких как внедрение подсказок и создание необоснованного контента. - [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

Ниже представлены ключевые идеи, которые определили программу AI Red Team компании Microsoft.AI red teaming теперь охватывает и безопасность, и результаты Responsible AI (RAI). Традиционно red teaming фокусировался на аспектах безопасности, рассматривая модель как вектор (например, кража базовой модели). Однако AI системы вводят новые уязвимости безопасности (например, внедрение подсказок, отравление данных), требующие особого внимания. Помимо безопасности, AI red teaming также проверяет проблемы справедливости (например, стереотипы) и вредоносный контент (например, глорификация насилия). Раннее выявление этих проблем позволяет определить приоритеты в области защиты.

Вот список дополнительной литературы, которая поможет вам лучше понять, как ред-тиминг может помочь выявить и смягчить риски в ваших системах искусственного интеллекта:
- [Планирование ред-тиминга для больших языковых моделей (LLM) и их применений](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [Что такое сеть ред-тиминга OpenAI?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI Red Teaming - ключевая практика для создания безопасных и ответственных решений в области искусственного интеллекта](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Адверсариальный угрозовой ландшафт для систем искусственного интеллекта)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), база знаний тактик и техник, используемых злоумышленниками в реальных атаках на системы искусственного интеллекта.

## Проверка знаний

Какой подход может быть хорошим для поддержания целостности данных и предотвращения их неправомерного использования?

1. Установить сильные контроли на основе ролей для доступа к данным и их управлению.
2. Внедрить и проверять маркировку данных, чтобы предотвратить искажение или неправомерное использование данных.
3. Убедиться, что ваша инфраструктура искусственного интеллекта поддерживает фильтрацию контента.

Ответ: 1. Все три рекомендации являются хорошими, но обеспечение назначения соответствующих привилегий доступа к данным пользователям значительно поможет предотвратить искажение и неправомерное представление данных, используемых LLM.

## 🚀 Задание

Узнайте больше о том, как вы можете [управлять и защищать конфиденциальную информацию](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst) в эпоху искусственного интеллекта.

## Отличная работа, продолжайте обучение

После завершения этого урока ознакомьтесь с нашей [коллекцией обучения по генеративному искусственному интеллекту](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), чтобы продолжить расширять свои знания о генеративном искусственном интеллекте!

Перейдите к Уроку 14, где мы рассмотрим [жизненный цикл применения генеративного искусственного интеллекта](../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)!
