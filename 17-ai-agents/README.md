## Введение

Агенты искусственного интеллекта представляют собой захватывающий прогресс в области генеративного искусственного интеллекта, позволяя большим языковым моделям (LLM) превратиться из помощников в агентов, способных принимать действия. Фреймворки агентов искусственного интеллекта позволяют разработчикам создавать приложения, которые предоставляют LLM доступ к инструментам и управлению состоянием. Эти фреймворки также повышают видимость, позволяя пользователям и разработчикам отслеживать планируемые действия LLM, тем самым улучшая управление опытом.

В уроке будут рассмотрены следующие аспекты:

- Понимание, что такое агент искусственного интеллекта - Что именно такое агент искусственного интеллекта?
- Исследование четырех различных фреймворков агентов искусственного интеллекта - Что делает их уникальными?
- Применение этих агентов искусственного интеллекта в различных сценариях использования - Когда следует использовать агентов искусственного интеллекта?

## Цели обучения

После прохождения этого урока вы сможете:

- Объяснить, что такое агенты искусственного интеллекта и как их можно использовать.
- Понимать различия между некоторыми популярными фреймворками агентов искусственного интеллекта и в чем их отличия.
- Понимать, как функционируют агенты искусственного интеллекта, чтобы создавать приложения с их использованием.

## Что такое агенты искусственного интеллекта?

Агенты искусственного интеллекта - это очень захватывающая область в мире генеративного искусственного интеллекта. Вместе с этим возникает иногда путаница в терминах и их применении. Чтобы все было просто и включало большинство инструментов, относящихся к агентам искусственного интеллекта, мы будем использовать следующее определение:

Агенты искусственного интеллекта позволяют большим языковым моделям (LLM) выполнять задачи, предоставляя им доступ к **состоянию** и **инструментам**.

![Модель агента](images/what-agent.png?WT.mc_id=academic-105485-koreyst)

Давайте определим эти термины:

**Большие языковые модели** - Это модели, на которые ссылается весь этот курс, такие как GPT-3.5, GPT-4, Llama-2 и т. д.

**Состояние** - Это относится к контексту, в котором работает LLM. LLM использует контекст своих прошлых действий и текущий контекст, направляя свое принятие решений для последующих действий. Фреймворки агентов искусственного интеллекта позволяют разработчикам легче поддерживать этот контекст.

**Инструменты** - Для выполнения задачи, которую запросил пользователь и которую LLM спланировал, LLM нужен доступ к инструментам. Некоторые примеры инструментов могут быть база данных, API, внешнее приложение или даже другая LLM!

Эти определения, надеюсь, помогут вам хорошо ориентироваться по мере продвижения и изучения их реализации. Давайте рассмотрим несколько различных фреймворков агентов искусственного интеллекта:

## LangChain Агенты

[LangChain Агенты](https://python.langchain.com/docs/modules/agents/?WT.mc_id=academic-105485-koreyst) - это реализация определений, которые мы предоставили выше.

Для управления **состоянием** используется встроенная функция под названием `AgentExecutor`. Она принимает определенный `агент` и доступные ему `инструменты`.

`AgentExecutor` также сохраняет историю чата, чтобы обеспечить контекст беседы.

![LangChain Агенты](images/langchain-agents.png?WT.mc_id=academic-105485-koreyst)

LangChain предлагает [каталог инструментов](https://integrations.langchain.com/tools?WT.mc_id=academic-105485-koreyst). Эти инструменты могут быть импортированы в ваше приложение, к которым может получить доступ LLM. Они создаются сообществом и командой LangChain.

Затем вы можете определить эти инструменты и передать их в `AgentExecutor`.

Видимость - еще один важный аспект, когда речь идет об агентах искусственного интеллекта. Важно, чтобы разработчики приложений понимали, какой инструмент использует LLM и почему. Для этого команда LangChain разработала LangSmith.

## AutoGen

Следующий фреймворк агентов искусственного интеллекта, о котором мы поговорим, - [AutoGen](https://microsoft.github.io/autogen/?WT.mc_id=academic-105485-koreyst). Основное внимание в AutoGen уделяется разговорам. Агенты являются **разговороспособными** и **настраиваемыми**.

**Разговороспособность** - LLM может начать и продолжить разговор с другим LLM для выполнения задачи. Это делается путем создания `AssistantAgents` и предоставления им конкретного системного сообщения.

```python

autogen.AssistantAgent( name="Разработчик", llm_config=llm_config, ) pm = autogen.AssistantAgent( name="Product_manager", system_message="Креативные идеи в подходах к разработке ПО.", llm_config=llm_config, )

```

**Настраиваемость** - Агенты могут быть определены не только как LLM, но также могут быть пользователем или инструментом. Как разработчик, вы можете определить `UserProxyAgent`, который отвечает за взаимодействие с пользователем для получения обратной связи при выполнении задачи. Эта обратная связь может либо продолжать выполнение задачи, либо останавливать ее.

```python
user_proxy = UserProxyAgent(name="user_proxy")
```

### Состояние и инструменты

Для изменения и управления состоянием, ассистент-агент генерирует Python-код для выполнения задачи.

Вот пример процесса:

![AutoGen](images/autogen.png?WT.mc_id=academic-105485-koreyst)  

#### LLM, определенный с системным сообщением

```python
system_message="Для задач, связанных с погодой, используйте только предоставленные вам функции. Выведи TERMINATE, когда задача выполнена."
```

Это системное сообщение указывает этому конкретному LLM, какие функции являются актуальными для его задачи. Помните, что с AutoGen вы можете иметь несколько определенных AssistantAgents с разными системными сообщениями.

#### Начало чата пользователем

```python
user_proxy.initiate_chat(chatbot, message="Я планирую поездку в Нью-Йорк на следующей неделе, можете помочь мне выбрать, что надеть?",)
```

Это сообщение от user_proxy (человека) запускает процесс работы агента для изучения возможных функций, которые следует выполнить.

#### Выполнение функции

```bash
chatbot (к user_proxy):

***** Предлагаемый вызов инструмента: get_weather ***** Аргументы: {"location":"Нью-Йорк, NY","time_period:"7","temperature_unit":"Цельсий"} ******************************************************** --------------------------------------------------------------------------------

>>>>>>>> ВЫПОЛНЯЕТСЯ ФУНКЦИЯ get_weather... user_proxy (к chatbot): ***** Ответ от вызова функции "get_weather" ***** 112.22727272727272 EUR ****************************************************************

```
После обработки первого чата агент отправляет предложение вызова инструмента. В данном случае это функция с названием `get_weather`. В зависимости от вашей конфигурации, эта функция может быть автоматически выполнена и прочитана агентом или может быть выполнена на основе ввода пользователя.

Вы можете найти список [примеров кода AutoGen](https://microsoft.github.io/autogen/docs/Examples/?WT.mc_id=academic-105485-koreyst), чтобы продолжить изучение того, как начать создание.

## Taskweaver

Следующий фреймворк агента, который мы рассмотрим, это [Taskweaver](https://microsoft.github.io/TaskWeaver/?WT.mc_id=academic-105485-koreyst). Он известен как "агент на основе кода", потому что вместо работы исключительно со строками он может работать с DataFrames в Python. Это становится чрезвычайно полезным для анализа данных и генерации задач. Это может быть такими вещами, как создание графиков и диаграмм или генерация случайных чисел.

### Состояние и инструменты

Для управления состоянием разговора TaskWeaver использует концепцию "Планировщика" (`Planner`). Планировщик - это LLM, который принимает запросы от пользователей и создает план выполнения задач, необходимых для выполнения этих запросов.

Для выполнения задач Планировщик использует набор инструментов, называемых "Плагины" (`Plugins`). Это могут быть классы Python или общий интерпретатор кода. Эти плагины хранятся в виде эмбеддингов, чтобы LLM мог лучше искать подходящий плагин.

![Taskweaver](images/taskweaver.png?WT.mc_id=academic-105485-koreyst)

Вот пример плагина для обработки обнаружения аномалий:

```python
class AnomalyDetectionPlugin(Plugin):
    def __call__(self, df: pd.DataFrame, time_col_name: str, value_col_name: str):
```

Код проверяется перед выполнением. Еще одна функция для управления контекстом в Taskweaver - это "опыт" (`experience`). Опыт позволяет сохранять контекст разговора в течение длительного времени в файле YAML. Это можно настроить таким образом, чтобы LLM постепенно улучшался в выполнении определенных задач, поскольку он имеет доступ к предыдущим разговорам.

## JARVIS

Последний фреймворк агента, который мы будем исследовать, - [JARVIS](https://github.com/microsoft/JARVIS?tab=readme-ov-file?WT.mc_id=academic-105485-koreyst). То, что делает JARVIS уникальным, заключается в том, что он использует LLM для управления `state` (состоянием) беседы, а `tools` (инструменты) - это другие модели искусственного интеллекта. Каждая из моделей искусственного интеллекта является специализированной моделью, выполняющей определенные задачи, такие как обнаружение объектов, транскрипция или подписывание изображений.

![JARVIS](images/jarvis.png?WT.mc_id=academic-105485-koreyst)

TLLM, будучи моделью общего назначения, получает запрос от пользователя и определяет конкретную задачу и любые аргументы/данные, необходимые для ее выполнения.

```python
[{"task": "object-detection", "id": 0, "dep": [-1], "args": {"image": "e1.jpg" }}]
```

Затем LLM форматирует запрос таким образом, чтобы специализированная модель искусственного интеллекта могла его интерпретировать, например, в формате JSON. Когда модель искусственного интеллекта возвращает свое предсказание на основе задачи, LLM получает ответ.

Если для выполнения задачи требуется несколько моделей, LLM также интерпретирует ответ от этих моделей, прежде чем объединить их для генерации ответа пользователю.

Приведенный ниже пример показывает, как это будет работать, когда пользователь запрашивает описание и подсчет объектов на изображении:

## Задание

Для продолжения изучения AI-агентов, которые можно создать с помощью AutoGen, вы можете выполнить следующее:

- Создайте приложение, которое симулирует деловую встречу с различными отделами стартапа в области образования.
- Создайте системные сообщения, которые помогут LLM-моделям понять различные персоны и приоритеты, а также позволят пользователю предложить новую идею продукта.
- LLM-модель должна затем генерировать дополнительные вопросы от каждого отдела для уточнения и улучшения предложения и идеи продукта.

## Обучение не останавливается здесь, продолжайте путешествие

После завершения этого урока ознакомьтесь с нашей [коллекцией по обучению генеративному искусственному интеллекту](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst), чтобы продолжить развитие ваших знаний о генеративном искусственном интеллекте!
